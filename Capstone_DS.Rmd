---
title: "Capstone Project Week 2 - Milestone Report"
author: "Divya Shree H P"
date: "3/2/2020"
output: html_document
---

## Basic summary

The goal of this project is provide a short overview of the exploratory analysis of the text data to be used for the Data Science Specialization - Capstone project. This document is concise and explain only the major features of the data identified and briefly summarizes the plans for creating the prediction algorithm and Shiny app.

Tasks to accomplish -

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app

## Loading source files and sampling

Data available to download from Coursera: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip


```{r warning=FALSE, message=FALSE}
# Loading packages

library(stringi)
library(knitr)
library(RWeka)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(dplyr)
library(ggplot2)

# Reading blogs, twitter, and news files

files = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt")

text <- list(blogs = "", news = "", twitter = "")

data_summary <- matrix(0, nrow = 3, ncol = 3, dimnames = list(c("blogs", "news", "twitter"),c("file size (Mb)", "lines", "words")))
for (i in 1:3) 
{
        con <- file(files[i], "rb")
        text[[i]] <- readLines(con, encoding = "UTF-8",skipNul = TRUE)
        close(con)
        data_summary[i,1] <- round(file.info(files[i])$size / 1024^2, 2)
        data_summary[i,2] <- length(text[[i]])
        data_summary[i,3] <- sum(stri_count_words(text[[i]]))
}
```
```{r}
kable(data_summary)
```

Due to very large data size and memory utilization, I will use 5% sample for analysis.

```{r}
set.seed(2468)
sample_blogs <- sample(text$blogs, round(length(text$blogs)*0.05,0))
sample_news <- sample(text$news, round(length(text$news)*0.05,0))
sample_twitter <- sample(text$twitter, round(length(text$twitter)*0.05,0))
```

```{r}
data <- c(sample_blogs, sample_news, sample_twitter)
sum <- sum(stri_count_words(data))
```

## Text Cleaning - Blogs

```{r}

corpus1 <- Corpus(VectorSource(sample_blogs))

# Convert to lower case, Remove punctuation marks / numbers / stop words / Whitespaces
data <- iconv(data, 'UTF-8', 'ASCII')
corpus <- Corpus(VectorSource(as.data.frame(data, stringsAsFactors = FALSE)))
corpus <- corpus %>%
  tm_map(tolower) %>%  
  tm_map(PlainTextDocument) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) %>%
  tm_map(removeWords, stopwords("english"))
```
```{r}
frequentWords <- head(sort(rowSums(as.matrix(TermDocumentMatrix(corpus))),decreasing=TRUE), 10)

barplot(frequentWords, 
        main = "Most Frequent Words", 
        xlab="Word", 
        ylab = "Count")
```

## Word Cloud

```{r}
term.doc.matrix1 <- TermDocumentMatrix(corpus)
term.doc.matrix1 <- as.matrix(term.doc.matrix1)
word.freqs1 <- sort(rowSums(term.doc.matrix1), decreasing=TRUE) 
dm1 <- data.frame(word=names(word.freqs1), freq=word.freqs1)
wordcloud(dm1$word, dm1$freq, min.freq= 100, random.order=TRUE, rot.per=.25, colors=brewer.pal(8, "Dark2"))
```


## Tokenization

```{r}
bigram <- NGramTokenizer(corpus, Weka_control(min = 2, max = 2))
trigram <- NGramTokenizer(corpus, Weka_control(min = 3, max = 3))
```

## Bigram frequency distribution

```{r}
bigram.df <- data.frame(table(bigram))
bigram.df <- bigram.df[order(bigram.df$Freq, decreasing = TRUE),]

ggplot(bigram.df[1:25,], aes(x=bigram, y=Freq)) +
  geom_bar(stat="Identity", fill="#0047AB")+
  xlab("Bigrams") + ylab("Frequency")+
  ggtitle("Most common 25 Bigrams") +
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

## Trigram frequency distribution

```{r}
trigram.df <- data.frame(table(trigram))
trigram.df <- trigram.df[order(trigram.df$Freq, decreasing = TRUE),]

ggplot(trigram.df[1:25,], aes(x=trigram, y=Freq)) +
  geom_bar(stat="Identity", fill="#0047AB")+
  xlab("Trigrams") + ylab("Frequency")+
  ggtitle("Most common 25 Trigrams") +
  theme(axis.text.x=element_text(angle=90, hjust=1))
```

## Future Analysis

1. Using the n-grams, a algorithm can be developed to suggest the next words in a text editor. Here, we can predict the probability of an untyped word from the frequencies in the corpus of the n-grams containing that word. Weighted sum of frequencies from machine learning can be used to build the predictive model.

2. Shiny Application will be used to use the predictive model built to show the probabilities of an untyped word.

